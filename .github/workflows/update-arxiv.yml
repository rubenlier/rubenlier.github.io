name: Update arXiv Papers

on:
  schedule:
    # Daily at midnight UTC
    - cron: "0 0 * * *"
    # Extra run later to catch arXiv indexing/announcement lag
    - cron: "0 6 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 requests

      # Robust query: try both "Ruben Lier" and "Lier, Ruben", newest-first.
      - name: Debug arXiv API fetch (runner) with retries
        env:
          ARXIV_QUERY: '(au:"Ruben Lier" OR au:"Lier, Ruben")'
          ARXIV_MAX_RESULTS: "100"
          ARXIV_SORTBY: "submittedDate"
          ARXIV_SORTORDER: "descending"
          ARXIV_USER_AGENT: "rubenlier.nl arXiv-scraper/1.0 (contact: ruben.lier@email.com)"
        run: |
          set -euo pipefail

          # URL-encode the query via python (avoids bash escaping footguns)
          Q_ENC="$(python - << 'PY'
          import os, urllib.parse
          q=os.environ["ARXIV_QUERY"]
          print(urllib.parse.quote(q, safe=""))
          PY
          )"

          URL="https://export.arxiv.org/api/query?search_query=${Q_ENC}&start=0&max_results=${ARXIV_MAX_RESULTS}&sortBy=${ARXIV_SORTBY}&sortOrder=${ARXIV_SORTORDER}"
          UA="$ARXIV_USER_AGENT"

          OUT_XML="debug_arxiv.xml"
          HDRS="debug_arxiv.headers"

          max_attempts=8
          base_sleep=3

          echo "ARXIV_QUERY=$ARXIV_QUERY"
          echo "URL=$URL"

          for attempt in $(seq 1 "$max_attempts"); do
            echo "Attempt $attempt/$max_attempts"

            code="$(curl -sS -L \
              -H "User-Agent: $UA" \
              -D "$HDRS" -o "$OUT_XML" \
              -w '%{http_code}' \
              "$URL" || echo "000")"

            echo "HTTP=$code"
            n="$(grep -c '<entry>' "$OUT_XML" || true)"
            echo "entry count=$n"

            if [[ "$code" == "200" && "$n" -gt 0 ]]; then
              echo "OK"
              break
            fi

            echo "---- HEAD $OUT_XML ----"
            head -n 120 "$OUT_XML" || true
            echo "---- END HEAD ----"

            if [[ "$code" == "429" || "$code" == "500" || "$code" == "502" || "$code" == "503" || "$code" == "504" || "$code" == "000" ]]; then
              retry_after="$(awk 'BEGIN{IGNORECASE=1} /^Retry-After:/ {gsub("\r",""); print $2}' "$HDRS" | tail -n 1)"
              if [[ -n "${retry_after:-}" ]]; then
                sleep_s="$retry_after"
              else
                exp=$(( base_sleep * (2 ** (attempt - 1)) ))
                jitter=$(( RANDOM % 3 ))   # 0â€“2s
                sleep_s=$(( exp + jitter ))
                if (( sleep_s > 120 )); then sleep_s=120; fi
              fi
              echo "Retrying after ${sleep_s}s..."
              sleep "$sleep_s"
              continue
            fi

            echo "Non-retriable HTTP status: $code"
            exit 1
          done

          if [[ "${code:-}" != "200" ]]; then
            echo "Failed after $max_attempts attempts (last HTTP=$code)"
            exit 1
          fi

          if [[ "$(grep -c '<entry>' "$OUT_XML" || true)" -eq 0 ]]; then
            echo "HTTP 200 but no <entry> elements found; refusing to continue."
            exit 1
          fi

      - name: Inspect feed (titles + updated dates)
        run: |
          python - << 'PY'
          import xml.etree.ElementTree as ET

          path="debug_arxiv.xml"
          ns={"a":"http://www.w3.org/2005/Atom"}
          root=ET.parse(path).getroot()

          entries=root.findall("a:entry", ns)
          print("n_entries:", len(entries))
          for e in entries[:15]:
              title=(e.findtext("a:title", default="", namespaces=ns) or "").strip().replace("\n"," ")
              updated=(e.findtext("a:updated", default="", namespaces=ns) or "").strip()
              published=(e.findtext("a:published", default="", namespaces=ns) or "").strip()
              aid=(e.findtext("a:id", default="", namespaces=ns) or "").strip()
              print("-", updated, "|", published, "|", title, "|", aid)
          PY

      - name: Show fetch_arxiv.py environment inputs
        env:
          ARXIV_QUERY: '(au:"Ruben Lier" OR au:"Lier, Ruben")'
          ARXIV_MAX_RESULTS: "100"
          ARXIV_SORTBY: "submittedDate"
          ARXIV_SORTORDER: "descending"
          ARXIV_USER_AGENT: "rubenlier.nl arXiv-scraper/1.0 (contact: ruben.lier@email.com)"
        run: |
          python - << 'PY'
          import os
          for k in ["ARXIV_USER_AGENT","ARXIV_QUERY","ARXIV_MAX_RESULTS","ARXIV_SORTBY","ARXIV_SORTORDER"]:
              print(f"{k}={os.getenv(k)}")
          PY

      - name: Run script to fetch arXiv papers
        env:
          ARXIV_USER_AGENT: "rubenlier.nl arXiv-scraper/1.0 (contact: ruben.lier@email.com)"
          # These envs are harmless even if your script doesn't read them yet;
          # add support in fetch_arxiv.py if desired.
          ARXIV_QUERY: '(au:"Ruben Lier" OR au:"Lier, Ruben")'
          ARXIV_MAX_RESULTS: "100"
          ARXIV_SORTBY: "submittedDate"
          ARXIV_SORTORDER: "descending"
        run: |
          set -euo pipefail
          python fetch_arxiv.py

      - name: Touch paper.html (force visible last-updated stamp)
        run: |
          set -euo pipefail
          TS="$(TZ=Europe/Amsterdam date '+%Y-%m-%d %H:%M (Amsterdam)')"

          # Keep a single visible line at the top of paper.html.
          # If it's already there, replace it; otherwise prepend it.
          if grep -q 'id="preprints-last-updated"' paper.html; then
            perl -0777 -i -pe 's#<div id="preprints-last-updated">.*?</div>#<div id="preprints-last-updated"><em>Last updated: '"$TS"'</em></div>#s' paper.html
          else
            tmp="$(mktemp)"
            {
              echo '<div id="preprints-last-updated"><em>Last updated: '"$TS"'</em></div>'
              cat paper.html
            } > "$tmp"
            mv "$tmp" paper.html
          fi

      - name: Commit and push if updated
        run: |
          set -euo pipefail
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"

          git add paper.html

          echo "---- git status ----"
          git status --porcelain || true
          echo "---- staged diff stat ----"
          git diff --cached --stat || true

          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Update arXiv papers in paper.html"
          git push
