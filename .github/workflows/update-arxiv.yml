name: Update arXiv Papers

on:
  schedule:
    - cron: "0 0 * * *"   # daily at midnight UTC
    - cron: "0 6 * * *"   # extra run for arXiv indexing lag
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Fetch arXiv Atom feed (robust query, newest-first) with retries
        env:
          ARXIV_QUERY: '(au:"Ruben Lier" OR au:"Lier, Ruben")'
          ARXIV_MAX_RESULTS: "100"
          ARXIV_SORTBY: "submittedDate"
          ARXIV_SORTORDER: "descending"
          ARXIV_USER_AGENT: "rubenlier.nl arXiv-scraper/1.0 (contact: ruben.lier@email.com)"
        run: |
          set -euo pipefail

          Q_ENC="$(python - << 'PY'
          import os, urllib.parse
          print(urllib.parse.quote(os.environ["ARXIV_QUERY"], safe=""))
          PY
          )"

          URL="https://export.arxiv.org/api/query?search_query=${Q_ENC}&start=0&max_results=${ARXIV_MAX_RESULTS}&sortBy=${ARXIV_SORTBY}&sortOrder=${ARXIV_SORTORDER}"
          UA="$ARXIV_USER_AGENT"

          OUT_XML="debug_arxiv.xml"
          HDRS="debug_arxiv.headers"

          max_attempts=8
          base_sleep=3

          echo "ARXIV_QUERY=$ARXIV_QUERY"
          echo "URL=$URL"

          for attempt in $(seq 1 "$max_attempts"); do
            echo "Attempt $attempt/$max_attempts"

            code="$(curl -sS -L \
              -H "User-Agent: $UA" \
              -D "$HDRS" -o "$OUT_XML" \
              -w '%{http_code}' \
              "$URL" || echo "000")"

            echo "HTTP=$code"
            n="$(grep -c '<entry>' "$OUT_XML" || true)"
            echo "entry count=$n"

            if [[ "$code" == "200" && "$n" -gt 0 ]]; then
              echo "OK"
              break
            fi

            if [[ "$code" == "429" || "$code" == "500" || "$code" == "502" || "$code" == "503" || "$code" == "504" || "$code" == "000" ]]; then
              retry_after="$(awk 'BEGIN{IGNORECASE=1} /^Retry-After:/ {gsub("\r",""); print $2}' "$HDRS" | tail -n 1)"
              if [[ -n "${retry_after:-}" ]]; then
                sleep_s="$retry_after"
              else
                exp=$(( base_sleep * (2 ** (attempt - 1)) ))
                jitter=$(( RANDOM % 3 ))
                sleep_s=$(( exp + jitter ))
                if (( sleep_s > 120 )); then sleep_s=120; fi
              fi
              echo "Retrying after ${sleep_s}s..."
              sleep "$sleep_s"
              continue
            fi

            echo "Non-retriable HTTP status: $code"
            head -n 120 "$OUT_XML" || true
            exit 1
          done

          if [[ "${code:-}" != "200" ]]; then
            echo "Failed after $max_attempts attempts (last HTTP=$code)"
            exit 1
          fi

          if [[ "$(grep -c '<entry>' "$OUT_XML" || true)" -eq 0 ]]; then
            echo "HTTP 200 but no <entry> elements found; refusing to continue."
            exit 1
          fi

      - name: Inspect feed (titles + updated dates)
        run: |
          python - << 'PY'
          import xml.etree.ElementTree as ET
          ns={"a":"http://www.w3.org/2005/Atom"}
          root=ET.parse("debug_arxiv.xml").getroot()
          entries=root.findall("a:entry", ns)
          print("n_entries:", len(entries))
          for e in entries[:20]:
              title=(e.findtext("a:title", default="", namespaces=ns) or "").strip().replace("\n"," ")
              updated=(e.findtext("a:updated", default="", namespaces=ns) or "").strip()
              published=(e.findtext("a:published", default="", namespaces=ns) or "").strip()
              aid=(e.findtext("a:id", default="", namespaces=ns) or "").strip()
              print("-", updated, "|", published, "|", title, "|", aid)
          PY

      - name: Generate paper.html from debug_arxiv.xml (authoritative)
        run: |
          set -euo pipefail
          python - << 'PY'
          import html
          import xml.etree.ElementTree as ET
          from datetime import datetime

          ns={"a":"http://www.w3.org/2005/Atom"}
          root=ET.parse("debug_arxiv.xml").getroot()

          def parse_dt(s: str) -> datetime:
              # Atom uses e.g. 2026-02-25T01:04:49Z
              return datetime.strptime(s, "%Y-%m-%dT%H:%M:%SZ")

          entries=[]
          for e in root.findall("a:entry", ns):
              title=(e.findtext("a:title", default="", namespaces=ns) or "").strip().replace("\n"," ")
              aid=(e.findtext("a:id", default="", namespaces=ns) or "").strip()
              published=(e.findtext("a:published", default="", namespaces=ns) or "").strip()
              updated=(e.findtext("a:updated", default="", namespaces=ns) or "").strip()
              authors=[(a.findtext("a:name", default="", namespaces=ns) or "").strip()
                       for a in e.findall("a:author", ns)]
              # Prefer abs link (aid is usually http://arxiv.org/abs/...)
              link = aid.replace("http://", "https://")
              entries.append({
                  "title": title,
                  "link": link,
                  "authors": authors,
                  "published": published,
                  "updated": updated,
              })

          # Sort newest-first by updated
          entries.sort(key=lambda x: parse_dt(x["updated"]), reverse=True)

          # Group by year of published date (this will create 2026, 2025, ...)
          groups={}
          for it in entries:
              y=parse_dt(it["published"]).year
              groups.setdefault(y, []).append(it)

          years=sorted(groups.keys(), reverse=True)

          def fmt_date(s: str) -> str:
              dt=parse_dt(s)
              # "25 February 2026"
              return dt.strftime("%d %B %Y").lstrip("0")

          out=[]
          out.append("<!doctype html>")
          out.append('<meta charset="utf-8">')
          out.append("<title>Preprints</title>")
          out.append("")  # keep file simple; your site CSS can style .paper
          # NOTE: the "last-updated" div is handled by the next step (touch)
          # but we include a placeholder so replacement works reliably.
          out.append('<div id="preprints-last-updated"><em>Last updated: (set by workflow)</em></div>')

          for y in years:
              out.append(f'<h2 style="margin-top:20px;border-bottom:2px solid #333;padding-bottom:5px;">{y}</h2>')
              for it in groups[y]:
                  title=html.escape(it["title"])
                  link=html.escape(it["link"])
                  authors=it["authors"] or []
                  authors_html=", ".join(html.escape(a) for a in authors) if authors else ""
                  out.append('<div class="paper">')
                  out.append(f'  <h3><a href="{link}" target="_blank" rel="noopener">{title}</a></h3>')
                  if authors_html:
                      out.append(f'  <p><strong>Authors:</strong> <strong>{authors_html}</strong></p>')
                  out.append(f'  <p><strong>Originally submitted:</strong> {html.escape(fmt_date(it["published"]))}</p>')
                  out.append("</div>")
                  out.append("<hr>")

          with open("paper.html", "w", encoding="utf-8") as f:
              f.write("\n".join(out) + "\n")

          print("Wrote paper.html with", len(entries), "entries across years:", years)
          PY

          echo "After generation:"
          ls -la paper.html
          echo "Top of paper.html:"
          head -n 40 paper.html

      - name: Assert new papers are present in paper.html (fail loudly if not)
        run: |
          set -euo pipefail
          MUST_IDS=("2602.21475" "2602.21367")
          for id in "${MUST_IDS[@]}"; do
            if grep -q "$id" paper.html; then
              echo "OK: found $id in paper.html"
            else
              echo "ERROR: missing $id in paper.html"
              echo "Showing any lines mentioning arxiv:"
              grep -n "arxiv" paper.html | head -n 80 || true
              exit 1
            fi
          done

      - name: Touch paper.html (force visible last-updated stamp)
        run: |
          set -euo pipefail
          TS="$(TZ=Europe/Amsterdam date '+%Y-%m-%d %H:%M (Amsterdam)')"
          if grep -q 'id="preprints-last-updated"' paper.html; then
            perl -0777 -i -pe 's#<div id="preprints-last-updated">.*?</div>#<div id="preprints-last-updated"><em>Last updated: '"$TS"'</em></div>#s' paper.html
          else
            tmp="$(mktemp)"
            {
              echo '<div id="preprints-last-updated"><em>Last updated: '"$TS"'</em></div>'
              cat paper.html
            } > "$tmp"
            mv "$tmp" paper.html
          fi

      - name: Commit and push if updated
        run: |
          set -euo pipefail
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"

          git add paper.html

          echo "---- staged diff stat ----"
          git diff --cached --stat || true

          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Update arXiv papers in paper.html"
          git push
