name: Update arXiv Papers

on:
  schedule:
    - cron: "0 0 * * *"   # daily at midnight UTC
    - cron: "0 6 * * *"   # extra run for arXiv indexing lag
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 requests

      - name: Debug arXiv API fetch (runner) with retries
        env:
          ARXIV_QUERY: '(au:"Ruben Lier" OR au:"Lier, Ruben")'
          ARXIV_MAX_RESULTS: "100"
          ARXIV_SORTBY: "submittedDate"
          ARXIV_SORTORDER: "descending"
          ARXIV_USER_AGENT: "rubenlier.nl arXiv-scraper/1.0 (contact: ruben.lier@email.com)"
        run: |
          set -euo pipefail

          Q_ENC="$(python - << 'PY'
          import os, urllib.parse
          print(urllib.parse.quote(os.environ["ARXIV_QUERY"], safe=""))
          PY
          )"

          URL="https://export.arxiv.org/api/query?search_query=${Q_ENC}&start=0&max_results=${ARXIV_MAX_RESULTS}&sortBy=${ARXIV_SORTBY}&sortOrder=${ARXIV_SORTORDER}"
          UA="$ARXIV_USER_AGENT"

          OUT_XML="debug_arxiv.xml"
          HDRS="debug_arxiv.headers"

          max_attempts=8
          base_sleep=3

          echo "ARXIV_QUERY=$ARXIV_QUERY"
          echo "URL=$URL"

          for attempt in $(seq 1 "$max_attempts"); do
            echo "Attempt $attempt/$max_attempts"

            code="$(curl -sS -L \
              -H "User-Agent: $UA" \
              -D "$HDRS" -o "$OUT_XML" \
              -w '%{http_code}' \
              "$URL" || echo "000")"

            echo "HTTP=$code"
            n="$(grep -c '<entry>' "$OUT_XML" || true)"
            echo "entry count=$n"

            if [[ "$code" == "200" && "$n" -gt 0 ]]; then
              echo "OK"
              break
            fi

            if [[ "$code" == "429" || "$code" == "500" || "$code" == "502" || "$code" == "503" || "$code" == "504" || "$code" == "000" ]]; then
              retry_after="$(awk 'BEGIN{IGNORECASE=1} /^Retry-After:/ {gsub("\r",""); print $2}' "$HDRS" | tail -n 1)"
              if [[ -n "${retry_after:-}" ]]; then
                sleep_s="$retry_after"
              else
                exp=$(( base_sleep * (2 ** (attempt - 1)) ))
                jitter=$(( RANDOM % 3 ))
                sleep_s=$(( exp + jitter ))
                if (( sleep_s > 120 )); then sleep_s=120; fi
              fi
              echo "Retrying after ${sleep_s}s..."
              sleep "$sleep_s"
              continue
            fi

            echo "Non-retriable HTTP status: $code"
            echo "---- HEAD $OUT_XML ----"
            head -n 120 "$OUT_XML" || true
            echo "---- END HEAD ----"
            exit 1
          done

          if [[ "${code:-}" != "200" ]]; then
            echo "Failed after $max_attempts attempts (last HTTP=$code)"
            exit 1
          fi

          if [[ "$(grep -c '<entry>' "$OUT_XML" || true)" -eq 0 ]]; then
            echo "HTTP 200 but no <entry> elements found; refusing to continue."
            exit 1
          fi

      - name: Inspect feed (titles + updated dates)
        run: |
          python - << 'PY'
          import xml.etree.ElementTree as ET
          ns={"a":"http://www.w3.org/2005/Atom"}
          root=ET.parse("debug_arxiv.xml").getroot()
          entries=root.findall("a:entry", ns)
          print("n_entries:", len(entries))
          for e in entries[:20]:
              title=(e.findtext("a:title", default="", namespaces=ns) or "").strip().replace("\n"," ")
              updated=(e.findtext("a:updated", default="", namespaces=ns) or "").strip()
              published=(e.findtext("a:published", default="", namespaces=ns) or "").strip()
              aid=(e.findtext("a:id", default="", namespaces=ns) or "").strip()
              print("-", updated, "|", published, "|", title, "|", aid)
          PY

      - name: Run script to fetch arXiv papers (with loud diagnostics)
        env:
          ARXIV_USER_AGENT: "rubenlier.nl arXiv-scraper/1.0 (contact: ruben.lier@email.com)"
          ARXIV_QUERY: '(au:"Ruben Lier" OR au:"Lier, Ruben")'
          ARXIV_MAX_RESULTS: "100"
          ARXIV_SORTBY: "submittedDate"
          ARXIV_SORTORDER: "descending"
        run: |
          set -euo pipefail

          echo "Repo root listing:"
          ls -la

          echo "Before: find paper.html"
          find . -maxdepth 3 -name "paper.html" -print -exec ls -la {} \; || true

          python fetch_arxiv.py

          echo "After fetch_arxiv.py: find paper.html"
          find . -maxdepth 3 -name "paper.html" -print -exec ls -la {} \; || true

          echo "Git status after fetch_arxiv.py:"
          git status --porcelain || true

          echo "Changed files (name-only):"
          git diff --name-only || true

      - name: Assert new papers are present in paper.html (fail loudly if not)
        run: |
          set -euo pipefail

          # Update these if you want different "must include" IDs
          MUST_IDS=("2602.21475" "2602.21367")

          if [[ ! -f "paper.html" ]]; then
            echo "ERROR: paper.html not found in repo root."
            echo "Here are any paper.html files found:"
            find . -name "paper.html" -print -maxdepth 5 || true
            exit 1
          fi

          for id in "${MUST_IDS[@]}"; do
            if grep -q "$id" paper.html; then
              echo "OK: found $id in paper.html"
            else
              echo "ERROR: missing $id in paper.html"
              echo "Showing first 80 lines of paper.html:"
              head -n 80 paper.html || true
              echo "Showing any lines mentioning arxiv.org:"
              grep -n "arxiv.org" paper.html | head -n 50 || true
              exit 1
            fi
          done

      - name: Touch paper.html (force visible last-updated stamp)
        run: |
          set -euo pipefail
          TS="$(TZ=Europe/Amsterdam date '+%Y-%m-%d %H:%M (Amsterdam)')"

          if grep -q 'id="preprints-last-updated"' paper.html; then
            perl -0777 -i -pe 's#<div id="preprints-last-updated">.*?</div>#<div id="preprints-last-updated"><em>Last updated: '"$TS"'</em></div>#s' paper.html
          else
            tmp="$(mktemp)"
            {
              echo '<div id="preprints-last-updated"><em>Last updated: '"$TS"'</em></div>'
              cat paper.html
            } > "$tmp"
            mv "$tmp" paper.html
          fi

      - name: Commit and push if updated
        run: |
          set -euo pipefail
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"

          git add paper.html

          echo "---- staged diff stat ----"
          git diff --cached --stat || true

          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Update arXiv papers in paper.html"
          git push
